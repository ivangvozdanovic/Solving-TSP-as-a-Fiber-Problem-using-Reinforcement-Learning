{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aece1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Ivan Gvozdanovic\n",
    "Date: 10/19/2023\n",
    "\n",
    "Finding optimal tour in Traveling Salesman Problem, using tabular Q-learning algorithm.\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b7803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fix the connectivity of the graph if we are picking multiple edges \n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d379028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.io\n",
    "from datetime import date, time, datetime as Date, time, datetime\n",
    "from scipy import optimize\n",
    "import networkx as nx\n",
    "import ast\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import time as Time\n",
    "from operator import itemgetter\n",
    "import math as m\n",
    "import copy as cpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "from TabularTspMultiEdgeSwapENV import PolytopeENV as Env\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49eaf72",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{Q-learning algorithm}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a57562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Q_learning import Q_learning, EdgeSwap_Q_learning, Multi_EdgeSwap_Q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3363c",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{Extracting the optimal policy}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13131918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimal_policy_extraction import optimal_policy_time_independent, \\\n",
    "                                      edge_swap_policy_evaluation, \\\n",
    "                                      average_optimal_policy, \\\n",
    "                                      compute_path_cost, \\\n",
    "                                      compute_state_radius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae168f3",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{Reward functions}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed39471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_functions import reward_cost, calculate_reward1, calculate_reward2, calculate_reward3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007cd51",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{Initial Solution}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d203cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path nodes: \n",
      " [0, 2, 7, 3, 1, 4, 5, 6]\n",
      "Path edges: \n",
      " [(0, 2), (2, 7), (7, 3), (3, 1), (1, 4), (4, 5), (5, 6), (6, 0)]\n",
      "sub nodes: \n",
      " [0, 2, 7, 3, 1, 4, 5, 6]\n",
      "sub sorted nodes: \n",
      " [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "sub edges: \n",
      " [(0, 2), (0, 6), (2, 7), (7, 3), (3, 1), (1, 4), (4, 5), (5, 6)]\n",
      "Initial solution: \n",
      " 28 [0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0]\n",
      "is connected:  True\n",
      "Is graph connected:  True\n",
      "Edges for reward list:  [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (3, 4), (3, 5), (3, 6), (3, 7), (4, 5), (4, 6), (4, 7), (5, 6), (5, 7), (6, 7)]\n",
      "Sub reward list: \n",
      " 28 [169, 83, 141, 40, 27, 145, 179, 15, 137, 60, 86, 71, 181, 117, 133, 190, 5, 171, 23, 159, 184, 48, 164, 19, 142, 37, 70, 45]\n",
      "Initial reward:  -845\n",
      "Margin: \n",
      " [2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "Number of actions is 20\n",
      "28 [0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0]\n",
      "[array([ 0,  1, -1,  0,  0,  0,  0, -1,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 0,  1,  0, -1,  0,  0,  0, -1,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 0,  1,  0,  0, -1,  0,  0, -1,  0,  0,  1,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 0,  1,  0,  0,  0, -1,  0, -1,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 0,  1,  0,  0,  0,  0, -1, -1,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  1,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  0,  0, -1,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  1,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  0,  0,  0, -1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  1,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  0,  0,  0,  0, -1,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  0,  0,  0,  0,  0, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  1, -1, -1,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  1, -1,  0, -1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  1, -1,  0,  0, -1,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  1, -1,  0,  0,  0, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  1,  0, -1, -1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0], dtype=int8), array([ 1,  1,  0, -1,  0, -1,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0], dtype=int8), array([ 1,  1,  0, -1,  0,  0, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0], dtype=int8), array([ 1,  1,  0,  0, -1, -1,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0], dtype=int8), array([ 1,  1,  0,  0, -1,  0, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0], dtype=int8), array([ 1,  1,  0,  0,  0, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1], dtype=int8)]\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import create_tsp_polytope_graph, extract_distance_matrix, create_state_graph, create_state_edges\n",
    "\n",
    "\n",
    "objective_table = [] # cost vector\n",
    "initial_states = {} # dictionary holding the initial states.\n",
    "reward_lists = [] # list holding the rewrds for each edge in the subproblem. We read it off to compute the travel cost.\n",
    "\n",
    "patches = 1\n",
    "nodes_per_patch = 8\n",
    "\n",
    "#Pick the file to the problem:\n",
    "file = 'TSP_MultiDiscrete_DQN'\n",
    "\n",
    "\n",
    "available_actions, initial_states, distance_matrix, objective_table, reward_list = create_tsp_polytope_graph(nodes_per_patch, \n",
    "                                                                                                             patches, \n",
    "                                                                                                             initial_states, \n",
    "                                                                                                             reward_lists,\n",
    "                                                                                                             file,\n",
    "                                                                                                             reward_cost,\n",
    "                                                                                                             1,200)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(initial_states[0]),initial_states[0])\n",
    "print(available_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d6fedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Optimal\n",
      "Optimal route: 0 -> 5  1 -> 2  2 -> 6  3 -> 4  4 -> 0  5 -> 1  6 -> 7  7 -> 3  \n",
      "Total Distance: 289.0\n",
      "Tour edges: \n",
      " [(0, 5), (1, 2), (2, 6), (3, 4), (4, 0), (5, 1), (6, 7), (7, 3)]\n",
      "Reward:  289\n"
     ]
    }
   ],
   "source": [
    "from exact_solution import TSP_integer_program\n",
    "\n",
    "tour_edges = TSP_integer_program(distance_matrix) # solve TSP-MZT integer program.\n",
    "print(\"Tour edges: \\n\", tour_edges)\n",
    "\n",
    "tour_edges = [(min(e),max(e)) for e in tour_edges]\n",
    "or_reward = 0\n",
    "for e in tour_edges:\n",
    "    or_reward += objective_table[e]\n",
    "print(\"Reward: \", or_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5df27c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_functions import multi_edge_swap_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e52ddf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "state = [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "state_edges = create_state_edges(8, state)\n",
    "action = multi_edge_swap_policy(None, state, state_edges, 1, 3)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c5868",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{Main training loop}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2004a4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (array([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "       1, 0, 0, 1, 0, 0], dtype=int32), -845)}\n",
      "[[0, 1, 2], [0, 1, 3], [0, 1, 4], [0, 1, 5], [0, 1, 6], [0, 1, 7], [0, 2, 3], [0, 2, 4], [0, 2, 5], [0, 2, 6], [0, 2, 7], [0, 3, 4], [0, 3, 5], [0, 3, 6], [0, 3, 7], [0, 4, 5], [0, 4, 6], [0, 4, 7], [0, 5, 6], [0, 5, 7], [0, 6, 7], [1, 2, 3], [1, 2, 4], [1, 2, 5], [1, 2, 6], [1, 2, 7], [1, 3, 4], [1, 3, 5], [1, 3, 6], [1, 3, 7], [1, 4, 5], [1, 4, 6], [1, 4, 7], [1, 5, 6], [1, 5, 7], [1, 6, 7], [2, 3, 4], [2, 3, 5], [2, 3, 6], [2, 3, 7], [2, 4, 5], [2, 4, 6], [2, 4, 7], [2, 5, 6], [2, 5, 7], [2, 6, 7], [3, 4, 5], [3, 4, 6], [3, 4, 7], [3, 5, 6], [3, 5, 7], [3, 6, 7], [4, 5, 6], [4, 5, 7], [4, 6, 7], [5, 6, 7]]\n"
     ]
    }
   ],
   "source": [
    "save_data = True  # save Q table data and cost vector data.\n",
    "save_plots = False  # save the plots\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "epsilon = 1  # exploration parameter.\n",
    "reward_parameter = 1\n",
    "lr = 0.07  # learning rate.\n",
    "discount_factor = 0.9  # discount parameter for the reward.\n",
    "episode_numbers = [1000]  # number of episodes we run the algorithm on.\n",
    "path_numbers = [20] # number of paths we run for each episode.\n",
    "max_path_lengths = [100] # maximum number of steps allowed per a path.\n",
    "table_size = available_actions[0].shape[0] # the size of each state and action vector.\n",
    "\n",
    "k_swaps = 3\n",
    "n_step_lookup = 10\n",
    "\n",
    "# Set the correct Q-learning configuration.\n",
    "episode_num = episode_numbers[0]\n",
    "path_num = path_numbers[0]\n",
    "show_path_num = path_num*50\n",
    "max_path_length = max_path_lengths[0]\n",
    "\n",
    "\n",
    "combinations = list(itertools.combinations(range(nodes_per_patch), 3))\n",
    "# Convert each tuple to a list\n",
    "action_space_values = [list(pair) for pair in combinations]\n",
    "action_space_size = nodes_per_patch-1\n",
    "\n",
    "lb = -2\n",
    "ub = 3\n",
    "\n",
    "best_states_size = 10\n",
    "best_states = {0: (initial_states[0], reward_cost(reward_list, initial_states[0]))}\n",
    "print(best_states)\n",
    "\n",
    "print(action_space_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8864293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:200: RuntimeWarning: invalid value encountered in cast\n"
     ]
    }
   ],
   "source": [
    "# Convert dictionary values to a list of arrays\n",
    "visited_states = [np.array(initial_states[0])]\n",
    "visited_states = np.stack(visited_states)\n",
    "\n",
    "\n",
    "#Initialize the environment.\n",
    "env = Env(initial_states[0], # initial_state\n",
    "         reward_list, # edge_weights\n",
    "         episode_num, # total_episodes\n",
    "         max_path_length,\n",
    "         50, # show_path_num\n",
    "         visited_states,  # visited_states\n",
    "         available_actions, # basis_moves\n",
    "         nodes_per_patch, # node_num\n",
    "         0, # P\n",
    "         best_states,\n",
    "         best_states_size,\n",
    "         objective_table,\n",
    "         False,\n",
    "         discount_factor,\n",
    "         reward_function = reward_cost\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9773eadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Episode Number:  1000 \n",
      " Path Number:  20 \n",
      " Learning Rates:  0.07 \n",
      " Gamma:  0.9 \n",
      " Table length:  28 \n",
      " Maximum Path Length:  100\n",
      "##############################  Episode:  1   ##############################\n",
      "[(0, 2), (0, 6), (1, 3), (1, 4), (2, 7), (3, 7), (4, 5), (5, 6)]\n",
      "[1, 2, 4]\n",
      "[(0, 6), (1, 3), (2, 7)]\n",
      "[(0, 2), (0, 3), (1, 4), (1, 7), (2, 6), (3, 7), (4, 5), (5, 6)]\n",
      "The BEST STATES DICT is full, and we found a better state, remove the worst state  10\n",
      "[(0, 2), (0, 3), (1, 4), (1, 7), (2, 6), (3, 7), (4, 5), (5, 6)]\n",
      "[0, 2, 5]\n",
      "[(0, 2), (1, 4), (3, 7)]\n",
      "[(0, 3), (0, 4), (1, 6), (1, 7), (2, 3), (2, 6), (4, 5), (5, 7)]\n",
      "The BEST STATES DICT is full, and we found a better state, remove the worst state  10\n",
      "[(0, 3), (0, 4), (1, 6), (1, 7), (2, 3), (2, 6), (4, 5), (5, 7)]\n",
      "[1, 3, 5]\n",
      "[(0, 4), (1, 7), (2, 6)]\n",
      "[(0, 3), (0, 7), (1, 6), (1, 7), (2, 3), (2, 4), (4, 5), (5, 6)]\n",
      "[(0, 3), (0, 7), (1, 6), (1, 7), (2, 3), (2, 4), (4, 5), (5, 6)]\n",
      "[0, 1, 6]\n",
      "[(0, 3), (0, 7), (4, 5)]\n",
      "[(0, 5), (0, 7), (1, 6), (1, 7), (2, 3), (2, 6), (3, 4), (4, 5)]\n",
      "The BEST STATES DICT is full, and we found a better state, remove the worst state  10\n",
      "[(0, 5), (0, 7), (1, 6), (1, 7), (2, 3), (2, 6), (3, 4), (4, 5)]\n",
      "[1, 4, 7]\n",
      "[(0, 7), (2, 3), (4, 5)]\n",
      "[(0, 3), (0, 5), (1, 6), (1, 7), (2, 5), (2, 6), (3, 4), (4, 7)]\n",
      "The BEST STATES DICT is full, and we found a better state, remove the worst state  10\n",
      "[(0, 3), (0, 5), (1, 6), (1, 7), (2, 5), (2, 6), (3, 4), (4, 7)]\n",
      "[1, 6, 7]\n",
      "[(0, 5), (3, 4), (4, 7)]\n",
      "[(0, 3), (0, 4), (1, 6), (1, 7), (2, 5), (2, 6), (3, 7), (4, 5)]\n",
      "[(0, 3), (0, 4), (1, 6), (1, 7), (2, 5), (2, 6), (3, 7), (4, 5)]\n",
      "[3, 4, 7]\n",
      "[(1, 7), (2, 5), (4, 5)]\n",
      "[(0, 3), (0, 4), (1, 5), (1, 6), (2, 4), (2, 6), (3, 7), (5, 7)]\n",
      "The BEST STATES DICT is full, and we found a better state, remove the worst state  10\n",
      "[(0, 3), (0, 4), (1, 5), (1, 6), (2, 4), (2, 6), (3, 7), (5, 7)]\n",
      "[2, 4, 7]\n",
      "[(1, 5), (2, 4), (5, 7)]\n",
      "[(0, 3), (0, 4), (1, 2), (1, 5), (1, 6), (2, 3), (2, 6), (5, 7)]\n",
      "The BEST STATES DICT is full, and we found a better state, remove the worst state  10\n",
      "[(0, 3), (0, 4), (1, 2), (1, 5), (1, 6), (2, 3), (2, 6), (5, 7)]\n",
      "[4, 5, 7]\n",
      "[(1, 6), (2, 3), (5, 7)]\n",
      "[(0, 3), (0, 4), (1, 2), (1, 3), (1, 5), (2, 6), (2, 7), (5, 6)]\n",
      "The BEST STATES DICT is full, and we found a better state, remove the worst state  10\n",
      "[(0, 3), (0, 4), (1, 2), (1, 3), (1, 5), (2, 6), (2, 7), (5, 6)]\n",
      "[0, 1, 6]\n",
      "[(0, 3), (0, 4), (2, 7)]\n",
      "[(0, 4), (0, 6), (1, 2), (1, 3), (1, 5), (2, 3), (2, 6), (5, 7)]\n",
      "The BEST STATES DICT is full, and we found a better state, remove the worst state  10\n",
      "[(0, 4), (0, 6), (1, 2), (1, 3), (1, 5), (2, 3), (2, 6), (5, 7)]\n",
      "[1, 6, 7]\n",
      "[(0, 6), (2, 6), (5, 7)]\n",
      "[(0, 4), (0, 6), (1, 2), (1, 3), (1, 5), (2, 3), (2, 7), (5, 6)]\n",
      "The BEST STATES DICT is full, and we found a better state, remove the worst state  10\n",
      "[(0, 4), (0, 6), (1, 2), (1, 3), (1, 5), (2, 3), (2, 7), (5, 6)]\n",
      "[0, 4, 5]\n",
      "[(0, 4), (1, 5), (2, 3)]\n",
      "[(0, 5), (0, 6), (1, 2), (1, 3), (2, 6), (2, 7), (4, 5)]\n",
      "The BEST STATES DICT is full, and we found a better state, remove the worst state  10\n",
      "[(0, 5), (0, 6), (1, 2), (1, 3), (2, 6), (2, 7), (4, 5)]\n",
      "[0, 3, 4]\n",
      "[(0, 5), (1, 3), (2, 6)]\n",
      "[(0, 3), (0, 6), (1, 2), (1, 6), (2, 5), (2, 7), (4, 5)]\n",
      "[(0, 3), (0, 6), (1, 2), (1, 6), (2, 5), (2, 7), (4, 5)]\n",
      "[0, 4, 6]\n",
      "[(0, 3), (2, 5), (4, 5)]\n",
      "GRAPH IS STILL DISCONNECTED\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvozd\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#Start the Q-learning algorithm.\n",
    "\n",
    "start_time = Time.time()\n",
    "\n",
    "Q = Multi_EdgeSwap_Q_learning(epsilon, \n",
    "                               episode_num, \n",
    "                               path_num, \n",
    "                               table_size, \n",
    "                               max_path_length, \n",
    "                               discount_factor, \n",
    "                               env, \n",
    "                               lr, \n",
    "                               save_plots, \n",
    "                               nodes_per_patch,\n",
    "                               action_space_values,\n",
    "                               action_space_size,\n",
    "                               n_step_lookup,\n",
    "                               k_swaps)\n",
    "\n",
    "\n",
    "\n",
    "#Save the Q table.\n",
    "if save_data:\n",
    "    time = Time.localtime()\n",
    "    current_time = Time.strftime(\"%H-%M-%S\", time)\n",
    "    date = datetime.now()\n",
    "    d = date.isoformat()[0:10]\n",
    "    data_save = [Q]\n",
    "    data_save = np.array(data_save, dtype=object)\n",
    "    print(d[0:10])\n",
    "    np.save('Models/'\n",
    "            +'Q_EP_'\n",
    "            +str(episode_num)+'_P_'+str(path_num)\n",
    "            +'_PL_'+ str(max_path_length)+'_Date_'+d+'_.npy',data_save)\n",
    "    \n",
    "\n",
    "    \n",
    "end_time = Time.time()\n",
    "\n",
    "print(f'It took {(end_time-start_time)/60} minutes to run {episode_num} episodes.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a194514",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{Examine the optimal policy}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load = np.load('Models/'\n",
    "                    +'Q_EP_'\n",
    "                    +str(episode_num)\n",
    "                    +'_P_'+str(path_num)\n",
    "                    +'_PL_'\n",
    "                    + str(max_path_length)\n",
    "                    +'_Date_2024-09-26'\n",
    "                    +'_.npy',allow_pickle=True)\n",
    "Q = data_load[0] \n",
    "\n",
    "with open('Models' + os.sep + 'visited_states.pkl', 'rb') as f:\n",
    "    starting_states = pickle.load(f)\n",
    "# print(starting_states, starting_states.shape)\n",
    "\n",
    "# #Initialize the environment.\n",
    "# env = Env(initial_states[0], # initial_state\n",
    "#          reward_list, # edge_weights\n",
    "#          episode_num, # total_episodes\n",
    "#          show_path_num, # show_path_num\n",
    "#          [initial_states[0]],  # visited_states\n",
    "#          available_actions, # basis_moves\n",
    "#          nodes_per_patch, # node_num\n",
    "#          0, # P\n",
    "#          lb, #lb\n",
    "#          reward_function = reward_cost,\n",
    "#          )\n",
    "\n",
    "\n",
    "# path, final_state, maximum_pair = optimal_policy_time_independent(Q,\n",
    "#                                                                 env,\n",
    "#                                                                 reward_list,\n",
    "#                                                                 table_size,\n",
    "#                                                                 available_actions,\n",
    "#                                                                 max_path_length,\n",
    "#                                                                 lb,\n",
    "#                                                                 ub,\n",
    "#                                                                 save_plots)\n",
    "\n",
    "\n",
    "# print(\"Path: \\n\", path)\n",
    "# print(\"Max pair:\", maximum_pair)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert dictionary values to a list of arrays\n",
    "# visited_states = [np.array(initial_states[0])]\n",
    "# visited_states = np.stack(visited_states)\n",
    "print(np.array([initial_states[0]]))\n",
    "trajectory_num = 10\n",
    "random_initial_states = starting_states[np.random.choice(starting_states.shape[0], trajectory_num, replace=False)]\n",
    "random_initial_states = np.concatenate((np.array([initial_states[0]]), random_initial_states), axis=0)\n",
    "\n",
    "\n",
    "for t in range(trajectory_num):\n",
    "    print(\"\\n\")\n",
    "    print(\"####################################################################################################\")\n",
    "    print(\"####################################################################################################\")\n",
    "    print(\"####################################################################################################\")\n",
    "    print(\"####################################################################################################\")\n",
    "    print(\"####################################################################################################\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Initial Solution: \\n\", random_initial_states[t,:])\n",
    "    #Initialize the environment.\n",
    "    env = Env(random_initial_states[t,:], # initial_state\n",
    "             reward_list, # edge_weights\n",
    "             episode_num, # total_episodes\n",
    "             100,\n",
    "             50, # show_path_num\n",
    "             starting_states,  # visited_states\n",
    "             available_actions, # basis_moves\n",
    "             nodes_per_patch, # node_num\n",
    "             0, # P\n",
    "             best_states,\n",
    "             best_states_size,\n",
    "             objective_table,\n",
    "             True,\n",
    "             discount_factor,\n",
    "             reward_function = reward_cost\n",
    "             )\n",
    "\n",
    "    edge_swap_policy_evaluation(Q,env,reward_list,20,action_space_values,action_space_size, nodes_per_patch)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ad50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
